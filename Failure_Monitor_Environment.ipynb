{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Failure_Monitor_Environment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7f+ZVYNEYFMzCc3I/mL0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christianadriano/RL_4_SelfHealingSystems/blob/master/Failure_Monitor_Environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6UDwMUFf04j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FailureMonitorEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        action :\n",
        "\n",
        "        Returns four outputs\n",
        "        -------\n",
        "        observation, reward, episode_over, cumulative_reward, info : tuple\n",
        "            observation (object) :\n",
        "                It is part of the internal state made visible to the agent\n".
        "                In our case, it is an ordered set of pairs <component, failure>, which is \n",
        "                initially created as a FIFO (first in first out) list\n",
        "                of failures that happened to different compoments, i.e., the\n",
        "                position of the pairs <component, failure> depends on when\n",
        "                the failure occurred. \n",
        "                Earlier failures are placed towards the end of the list, while\n",
        "                more recent failures are at the beginning of the list. This list\n",
        "                can be re-ordered by using the swap action (see next).\n",
        "            reward (float) :\n",
        "                if action==repair, then returns a utility increase for the\n",
        "                corresponding <component, failure> pair. \n",
        "                Constrain: only the top of the list can be repaired, hence if\n",
        "                the component to be repaired is not at the top of the \n",
        "                list, then, the agent has to call for a swap action.\n",
        "\n",
        "                if action==swap, if successful, swap the two components places\n",
        "                and returns the cost of doing the swap. \n",
        "                \n",
        "                The total reward that has to me maximize will be kept up-to-date\n",
        "                by another class.\n",
        "            cumulative_reward (float): Scalar value that is the product of two vectors\n",
        "                [Repair_Cycle] x [Utility_Increase]. Repair_Cycle keeps for each compoment\n",
        "                the information of how many cycles have passed since the component was\n"
        "                successfully repaired. The Utility_Increase values must be the same that were\n"
        "                sampled at the time of the repair.\n"
        "            episode_over (bool) :\n",
        "                Tells whether it is time to reset the environment again. The \n",
        "                episodes are automacally over when we emptied the list of tuples\n",
        "                <component,failure>. Hence, TRUE indicates that the episode has \n",
        "                terminated.\n",
        "            info (dict) :\n",
        "                 Diagnostic information useful for debugging. \n",
        "                 We can report here the transition matrix used, the full table\n",
        "                 of <compoment, failure, utility_increase> tuples.\n",
        "\n",
        "                 However, note that official evaluations of our agent should not \n",
        "                 use this internal information for learning.\n",
        "        \"\"\"\n",
        "        self._take_action(action)\n",
        "        self.status = self.env.step()\n",
        "        reward = self._get_reward()\n",
        "        ob = self.env.getState()\n",
        "        episode_over = self.status != hfo_py.IN_GAME\n",
        "        return ob, reward, episode_over, {}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNTngNaNtWN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _reset(self):\n",
        "        pass\n",
        "\n",
        "def _render(self, mode='human', close=False):\n",
        "        pass\n",
        "\n",
        "def _take_action(self, action):\n",
        "        pass\n",
        "\n",
        "def _get_reward(self):\n",
        "        \"\"\" Reward is given for XY. \"\"\"\n",
        "        if self.status == C1_F1:\n",
        "            return 1\n",
        "        elif self.status == C2_F2\n",
        "            return 2\n",
        "        else:\n",
        "            return -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
